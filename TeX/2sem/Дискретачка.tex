\documentclass{book}
%nerd stuff here
\pdfminorversion=7
\pdfsuppresswarningpagegroup=1
% Languages support
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel}
% Some fancy symbols
\usepackage{textcomp}
\usepackage{stmaryrd}
% Math packages
\usepackage{amsmath, amssymb, amsthm, amsfonts, mathrsfs, dsfont, mathtools}
\usepackage{cancel}
% Bold math
\usepackage{bm}
% Resizing
%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
% Optional font for not math-based subjects
%\usepackage{cmbright}

\author{Коченюк Анатолий}
\title{Конспект по дискретной математике\\ II семестр}

\usepackage{url}
% Fancier tables and lists
\usepackage{booktabs}
\usepackage{enumitem}
% Don't indent paragraphs, leave some space between them
\usepackage{parskip}
% Hide page number when page is empty
\usepackage{emptypage}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{xcolor}
% Some shortcuts
\newcommand\N{\ensuremath{\mathbb{N}}}
\newcommand\R{\ensuremath{\mathbb{R}}}
\newcommand\Z{\ensuremath{\mathbb{Z}}}
\renewcommand\O{\ensuremath{\emptyset}}
\newcommand\Q{\ensuremath{\mathbb{Q}}}
\renewcommand\C{\ensuremath{\mathbb{C}}}
\newcommand{\p}[1]{#1^{\prime}}
\newcommand{\pp}[1]{#1^{\prime\prime}}
\newcommand{\tl}[1]{\widetilde{#1}}
\DeclareMathOperator{\Reg}{Reg}
\DeclareMathOperator{\Aut}{Aut}
% Easily typeset systems of equations (French package) [like cases, but it aligns everything]
\usepackage{systeme}
\usepackage{lipsum}
% limits are put below (optional for int)
\let\svlim\lim\def\lim{\svlim\limits}
\let\svsum\sum\def\sum{\svsum\limits}
%\let\svlim\int\def\int{\svlim\limits}
% Command for short corrections
% Usage: 1+1=\correct{3}{2}
\definecolor{correct}{HTML}{009900}
\newcommand\correct[2]{\ensuremath{\:}{\color{red}{#1}}\ensuremath{\to }{\color{correct}{#2}}\ensuremath{\:}}
\newcommand\green[1]{{\color{correct}{#1}}}
% Hide parts
\newcommand\hide[1]{}
% si unitx
\usepackage{siunitx}
\sisetup{locale = FR}
% Environments
% For box around Definition, Theorem, \ldots
\usepackage{mdframed}
\mdfsetup{skipabove=1em,skipbelow=0em}
\theoremstyle{definition}
\newmdtheoremenv[nobreak=true]{definition}{Определение}
\newmdtheoremenv[nobreak=true]{theorem}{Теорема}
\newmdtheoremenv[nobreak=true]{lemma}{Лемма}
\newmdtheoremenv[nobreak=true]{problem}{Задача}
\newmdtheoremenv[nobreak=true]{property}{Свойство}
\newmdtheoremenv[nobreak=true]{statement}{Утверждение}
\newmdtheoremenv[nobreak=true]{corollary}{Следствие}
\newtheorem*{note}{Замечание}
\newtheorem*{example}{Пример}
\renewcommand\qedsymbol{$\blacksquare$}
% Fix some spacing
% http://tex.stackexchange.com/questions/22119/how-can-i-change-the-spacing-before-theorems-with-amsthm
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=\parskip \thm@postskip=0pt
}
\usepackage{xifthen}
\def\testdateparts#1{\dateparts#1\relax}
\def\dateparts#1 #2 #3 #4 #5\relax{
    \marginpar{\small\textsf{\mbox{#1 #2 #3 #5}}}
}

\def\@lecture{}%
\newcommand{\lecture}[3]{
    \ifthenelse{\isempty{#3}}{%
        \def\@lecture{Lecture #1}%
    }{%
        \def\@lecture{Lecture #1: #3}%
    }%
    \subsection*{\@lecture}
    \marginpar{\small\textsf{\mbox{#2}}}
}
% Todonotes and inline notes in fancy boxes
\usepackage{todonotes}
\usepackage{tcolorbox}

% Make boxes breakable
\tcbuselibrary{breakable}
\newenvironment{correction}{\begin{tcolorbox}[
    arc=0mm,
    colback=white,
    colframe=green!60!black,
    title=Correction,
    fonttitle=\sffamily,
    breakable
]}{\end{tcolorbox}}
% These are the fancy headers
\usepackage{fancyhdr}
\pagestyle{fancy}

% LE: left even
% RO: right odd
% CE, CO: center even, center odd
% My name for when I print my lecture notes to use for an open book exam.
% \fancyhead[LE,RO]{Gilles Castel}

\fancyhead[RO,LE]{\@lecture} % Right odd,  Left even
\fancyhead[RE,LO]{}          % Right even, Left odd

\fancyfoot[RO,LE]{\thepage}  % Right odd,something additional 1  Left even
\fancyfoot[RE,LO]{}          % Right even, Left odd
\fancyfoot[C]{\leftmark}     % Center

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\newcommand{\incfig}[1]{%
    \def\svgwidth{\columnwidth}
    \import{./figures/}{#1.pdf_tex}
}
\usepackage{tikz}
\begin{document}
    \maketitle
    \chapter{Дискретная теория вероятностей}
    \section{Введение}

    \begin{definition}
        [Вероятностное пространство]
        $ $\\
        $\Omega$ -- элементарные исходы, неделимые дальше.

         $p$ -- дискретная плотность вероятности.

         $p: \Omega \to [0,1]\quad \sum\limits_{q\in \Omega} p(\omega) = 1$
    \end{definition}

    \begin{note}
        В случае дискретного вероятностного пространства $\left| \Omega \right| $ -- не более, чем счётное.
    \end{note}

    \begin{example}
        [Честная монета]
        $\Omega = \{0,1\}\quad p(0) = p(1) = \frac{1}{2}$
    \end{example}

    \begin{example}
        [Нечестная монета]
        $\Omega = \{0,1\}\quad p(1) = p, p(0) = q$ -- различные числа. $p+q = 1$

        Ещё одно название -- \underline{распределение Бернулли}
    \end{example}

    \begin{example}
        [Честная игральная кость]
        $\Omega = \{1, 2, 3, 4, 5, 6\}\quad p(\omega) = \frac{1}{6}$
    \end{example}

    \begin{definition}
        Событие, случайное событие -- $A\subseteq \Omega$
    \end{definition}

    \begin{note}
        Неправильное определение -- то, что может произойти, а может не произойти.

            $\O \subseteq \Omega\quad \Omega\subseteq \Omega$ -- примеры, когда никогда не происходит и всегда происходит
    \end{note}
    \begin{note}
        Для недискретного случая неверно, что \underline{любое} подмножество $\Omega$ это событие
    \end{note}

    \begin{definition}
        Вероятность события $P(A) = \sum\limits_{\omega\in A} p(\omega)$

        $p$ берёт элементарные исходы.  $P, \mathbb{P}$ -- вероятность события
    \end{definition}

    \begin{example}
        
        Событие $E = \{2, 4, 6\}\quad P(E) = p(2) + p(4) + p(6) = \frac{3}{6} = \frac{1}{2}$

        $O = \{1, 3, 5\}$
    \end{example}

    \begin{note}
        Не существует вероятностного пространства с бесконечным числом равновероятных исходов


        $p(\omega) = 0\quad \sum = 0$

        $p(\omega) = a >0\quad \sum = a\cdot (+\infty ) = +\infty $
    \end{note}

    \begin{example}
        Событие $B(IG) = \{4, 5, 6\} \quad P(B) = \frac{1}{2}$
    \end{example}

    \begin{definition}
        [Независимое событие] События $A, B$ независимы, если  $P(A\cap B) = P(A) \cdot  P(B)$
    \end{definition}

    \begin{example}
        $E\cap O = \O \quad B\cap E = \{4,6\}$

        $P(E\cap O) = \O \quad P(O)\cdot P(B) = \frac{1}{4}\neq 0$ 

        $P(B)\cdot P(E) = \frac{1}{4} \neq \frac{1}{3} = P(B\cap E)$
    \end{example}

    \begin{tikzpicture}
        \draw (0,0) circle [radius = 2];
        \draw[fill = green, opacity = 0.5] (-0.5,0) circle [radius = 0.8];
        \draw[fill = red, opacity = 0.5] (0.5,0) circle [radius = 0.8];
    \end{tikzpicture}

    $P(A\cap B) = P(A)\cdot P(B)$

    $\frac{P(A\cap B)}{P(B)} = \frac{P(A)}{P(\Omega)}$

    \begin{definition}
        [Условная вероятность] 
        $P(A|B) = \frac{P(A\cap B)}{P(B)}$
    \end{definition}
    \begin{note}
        Альтернативное определение независимости, не поддерживающее 0: $P(A | B) = P(A)$
    \end{note}

    $V = \{5,6\}$

    $P(V\cap E) = \frac{1}{6}$ 

    $P(V) = \frac{1}{3}\quad P(E) = \frac{1}{2}\quad P(V) \cdot  P(E) = \frac{1}{3}\cdot \frac{1}{2} = \frac{1}{6} = P(V\cap E)$

    \begin{definition}
        [Произведение вероятностных пространств]
        $ $\\
        $\Omega_1, p_1\qquad \Omega_2, p_2$

        $\Omega = \Omega_1 \times \Omega_2$

        $p\left( \left<\omega_1, \omega_2 \right> \right)  = p_1\left( \omega_1 \right) \cdot p_2(\omega_2)$
    \end{definition}
    \begin{theorem}
        $\forall A_1\subseteq \Omega_1$ и $\forall  A_2\subseteq \Omega_2$

        $A_1\times \Omega_2$ и $\Omega_1\times A_2$ -- независимы
    \end{theorem}
    \begin{proof}
        $P\left( A_1\times \Omega_2 \cap \Omega_1\times A_2 \right)  = P\left( A_1\times A_2 \right) = \sum\limits_{\substack{a\in A_1\\b\in A_2}} p\left( \left<a, b \right> \right)  = \\ = \sum\limits_{a\in A_1} \sum\limits_{b\in A_2} p_1(a)\cdot p_2(b) = \sum\limits_{a\in A_1}p_1(a)\left( \sum\limits_{b\in A_2} p_2(b) \right) = P_1(A_1)\cdot P_2(A_2)$
    \end{proof}

    \begin{definition}
        $A_1, A_2, \ldots, A_n$

        \begin{enumerate}
            \item Попарно независимые $\quad A_i$ и $A_j$ независимы
            \item Независимы в совокупности $\forall I\subseteq \{1, 2, \ldots, n\}~ P\left( \bigcap\limits_{i\in I}A_i \right)  = \prod\limits_{i\in I} P(A_i)$

                $P(A_1\cap A_2\cap A_3) = P(A_1)\cdot P(A_2)\cdot P(A_3)$
        \end{enumerate}
    \end{definition}

    \begin{example}
        Кидаем две монеты $\Omega = \{00, 01, 10, 11\}$

        $A_1 = \{10,11\}\quad A_2 = \{01,11\}\quad A_3 = \{01,10\}$ -- независимы попарно, но не в совокупности
    \end{example}

    \begin{definition}
        [Формула полной вероятности]
        $ $\\
        $\Omega = A_1\cup A_2\cup \ldots \cup A_n\quad i\neq j\implies A_i\cap A_j = \O $

        Совокупность таких А-шек называется полной системой событий.

        Дано: вероятности $P(A_i)\quad P(B|A_i)$
        Найти: $P(B)$

        \[P(B) = \sum_{i=1}^nP(B\cap A_i) = \sum_{i=1}^n P(B|A_i)\cdot P(A_i)\] -- формула полной вероятности

        Найти: $P\left( A_j|B \right) $

        $A_1$ -- болен, $A_2$ -- здоров, $B$ -- положительный результат теста

        $P(A_2|B)$

        \[P(A_j|B) = \frac{P(A_j\cap B)}{P(B)} = \frac{P(B|A_j)\cdot P(A_j)}{\sum_{i=1}^nP\left( B|A_i \right)\cdot P(A_i) }\] -- формула Байеса
    \end{definition}
\begin{figure}[ht]
    \centering
    \incfig{b}
    \caption{B}
    \label{fig:b}
\end{figure}

\section{Случайные величины}

\begin{note}
    Неправильное (наивное) определение -- величина, принемающая слуйное значение.

    Она может быть константой. Что такое величина?
\end{note}

\begin{definition}
    [Случайная величина]

    $\xi: \Omega \to \R$ -- $\R$ -значная функция

    $\Omega, p$ -- вероятностное пространство. 
\end{definition}

\begin{example}
    Если взять случайные текст длинной 1Кб. Вариантов текста очень много и бессмысленно их рассматривать отдельно, интересует какое-то свойство, величина.

    Графы, $2^{n \choose 2}$ штук. Но нас интересует какая-то (численная) характеристика элементарного исхода.
\end{example}

\begin{example}
    $D(ice) = \{1, 2, 3, 4, 5, 6 \}$

    $\Omega = D^2\quad p\left( \left<i, j \right> \right) =\frac{1}{36}$ 

    $\xi: \Omega \to  \R\qquad \xi(\left<i, j \right>) = i+j$
\end{example}

\begin{example}
    [Случайные графы]

    $G(4, \frac{1}{2})$ -- случайный граф, 4 вершины, каждое ребро существует с вероятностью $\frac{1}{2}$ 

    $\Omega = \mathds{B}^6\quad p(G) = \frac{1}{64}$

    $\xi(G)$ = количество компонент связности
\end{example}

\begin{example}
    $\Omega = \{1, 2, 3, 4, 5,6\}$
$\xi(w) = w$
\end{example}
\begin{example}
    $\Omega = \{1, 2, 3, 4, 5, 6\}\quad E = \{2, 4, 6\}$

    $\chi_E(\omega) = \begin{cases}
        1,& \omega\in E\\
        0,&\omega \not\in E\\
    \end{cases}$ -- индикаторная случайная величина
\end{example}

\begin{definition}
    $\Omega, p\quad \xi$

    $\left[ \xi = i \right]  = \{\omega |  \xi(\omega) = i\}\subseteq \Omega$

    $P\left( \left[ \xi = i \right]  \right)  = P\left( \xi = i \right)  = f_{\xi}(i)\quad f:\R\to \R$

    $f_{\xi}(i) = P\left( \xi = i \right) $ -- дискретная плотность вероятности случайной величины $\xi$

    $F_{\xi}(i):\R\to \R = P\left( \xi\leqslant i \right) $ -- функция распределения


\end{definition}

\begin{note}

    Непрерывная vs Дискретная вероятность
    
\begin{figure}[ht]
    \centering
    \incfig{непрерывная-вероятность}
    \caption{непрерывная и дискретная вероятность}
    \label{fig:непрерывная-вероятность}
\end{figure}
\end{note}

\begin{example}
\begin{figure}[ht]
    \centering
    \incfig{два-кубика}
    \caption{Два-кубика (функция распределения)}
    \label{fig:два-кубика}
\end{figure}
\end{example}


\begin{note}
    $\delta(x) = \begin{cases}
        0, &x\neq 0\\
        +\infty ,&x=0\\
    \end{cases}$ 

    $\int\limits_{-\infty }^{+\infty }\delta(x)dx = 1$

    $f_{\xi}(i) = P\left( \xi = i \right) \qquad f_{\xi} = \int\limits_{-\infty }^{+\infty }f_{xi}(x) = F_{\xi}(i)$

    $f_{\xi}(x) = \sum_i P\left( \xi = i \right) f\left( x-i \right) $
\end{note}

\begin{example}
    $\Omega = \mathbb{B}^{1000}\quad p(\omega) = \frac{1}{2^{1000}}$ 

    $\xi(w)$ =  число 1 в  $\omega$

    $\left| \text{множество значений } \xi \right|  = 1001\qquad p\left( \xi = i \right) = \frac{{1000\choose i}}{2^{1000}}$

\end{example}

\begin{note}
    Случайные числа обозначаются строчными греческими или заглавными латинскими из конца алфавита (X, Z)
\end{note}

\begin{note}
    [Что можно делать со случайными величинами]

    $\xi, \eta$ -- функции

    $\xi^2\quad 2\xi\quad \xi+\eta\quad \xi\cdot \eta\quad \xi^{\eta}\quad \sin \xi\quad e^{\eta}\quad \frac{1+\xi}{\eta}$ (всё то же, что мы можем делать с функциями.
\end{note}

\begin{example}
    $\Omega = D^2\quad \xi_1\left( \left<i, j \right> \right)  = i\qquad \xi_2\left( \left<i, j \right> \right) = j $ -- одинаково распределённые случайные величины
\end{example}

\begin{example}
    $\Omega = F\quad id(\omega) = \omega$

    $1, 2, \ldots, 6$ -- каждый с вероятностью $\frac{1}{6}$. Другое вероятностное пространство относительно предыдущего примера, но всё равно одинаковое распределение.

    $\xi = (i+j) = \xi_1 + \xi_2\qquad \xi = (i+j)\%6 +1$ -- у второй то же распределение, что у верхних, но она уже совсем другая.
\end{example}

\begin{definition}
    Математическое ожидание \[
        E_{\xi} = \sum_{\omega}p\left( \omega \right) \xi(\omega)
    .\] 

\end{definition}
\begin{statement}
    $E_{\xi} = \sum_i ip(\xi = i)$ 
\end{statement}
\begin{proof}
     \begin{align*}
         E_{\xi} &= \sum_{\omega}p\left( \omega \right) \xi\left( \omega \right)  \\
                 &= \sum_i\sum_{w:\xi(\omega) = i}p\left( \omega \right) \cdot  \xi\left(\omega  \right)\\
                 &= \sum_i\sum_{\omega:\xi\left( \omega \right) =i}p\left( \omega \right) \cdot i \\
                 &= \sum_i i\negthickspace\negthickspace\sum_{w:\xi\left( \omega \right) =i} p\left( \omega \right) \\
                 &= \sum_i iP\left( \xi = i \right)  \\
    .\end{align*}
\end{proof}

\begin{example}
    $\Omega = D\quad \xi = id$

    $E_{\xi} = \frac{1}{6}\cdot 1 + \frac{1}{6}\cdot 2 + \frac{1}{6}\cdot 3 + \frac{1}{6}\cdot 4 + \frac{1}{6}\cdot 5 + \frac{1}{6}\cdot 6 = \frac{21}{6} = \frac{7}{2} = 3,5$
\end{example}

\begin{example}
    $\Omega = D^2\quad \xi \left( \left<i, j \right> \right) = i+j$

    $E_{\xi} = \frac{1}{36}\cdot \left( 2\cdot 1+3\cdot 2+4\cdot 3+5\cdot 4+6\cdot 5+7\cdot 6+8\cdot 5+9\cdot 4+10\cdot 3+11\cdot 2+12\cdot 1 \right) $ -- здесь среднее значение оказалось наиболее частым, но так оно не всегда (пример с 3,5)
\end{example}


\begin{theorem}
    $E_{\lambda\xi} = \lambda E_{\xi}$

    $E\left( \xi + \eta \right)  = E_{\xi} + E_{\eta}$
\end{theorem}
\begin{proof}
    $E_{\lambda\xi} = \sum_{\omega}p\left( \omega \right) \lambda\xi\left( \omega \right)  = \lambda E_{\xi}$

    $E\left( \xi + \eta \right)  = \sum_{\omega  }p\left( \omega \right) \left( \xi\left( \omega \right)  + \eta\left( \omega \right)  \right) = E_{\xi} + E_{\eta}$
\end{proof}

\begin{statement}
    Если $\xi$ и  $\eta$ одинаково распределены, то  $E_{\xi} = E_{\eta}$
\end{statement}

\begin{example}
    Бросим кубик один раз, $\xi_1$ -- что выпало сверху, $\xi_2$ -- что выпало снизу

    $E\left( \xi_1 + \xi_2 \right)  = 7$. -- не играет роли как числа друг относительно друга расположены.
\end{example}

\begin{center}
    {\Huge \textbf{ \color{red} МАТОЖИДАНИЕ ЛИНЕЙНО ВСЕГДА}}
    
\end{center}

\begin{example}
    $\Omega = S_n\quad p(\omega) = \frac{1}{n!}$ 

    $\xi\left( \pi  \right)  = \left| \{i | \pi [i] = i\} \right| \quad 0\ldots n$, кроме $n-1$

    $E_{\xi} = \sum_{j=1}^{n} \xi_i = 1$

    $\xi_i\left( \pi  \right)  = \begin{cases}
        1, & \pi [i] = i\\
        0, & \text{ иначе}
    \end{cases}$ 

    $E_{\xi_i} = \frac{1}{n}$ 

    $\xi = \sum_{j=1}^{n} \xi_i$

\end{example}
\section{Независимые случайные величины}

\begin{definition}
    [удобное]
    Случайные величины  $\xi$ и  $\eta$ независимы, если события  $[\xi = \alpha]$ и $\left[ \eta = \beta \right] $ -- независимы $\forall \alpha, \beta$
\end{definition}

\begin{definition}
    [нормальное]

    $[\xi\leqslant \alpha]$ и $\left[ \eta\leqslant \beta \right] $ -- независимы для $\forall \alpha, ]beta$
\end{definition}

\begin{example}
    $\Omega = \Omega_1 \times  \Omega_2$

    $\xi_1\left( \left<\omega_1, \omega_2 \right> \right)  = f\left( \omega_1 \right) $

    $\xi_2\left( \left<\omega_1, \omega_2 \right> \right) = g\left( \omega_2 \right) $

    $A$ и  $B$ независимы, $\chi_A, \chi_B$ -- независимы
\end{example}

\begin{theorem}
    $\xi, \eta$ -- независимы  $\implies E\left( \xi\cdot \eta \right)  = E_{\xi} \cdot  E_{\eta}$
\end{theorem}
\begin{proof}
    $E \xi\cdot \eta = \sum_{\alpha} \alpha\cdot P\left( \xi\cdot \eta = \alpha \right)  = \sum_{i, j}\alpha P\left( \left[ \xi = i \right] \cap  \left[ \eta = j \right]  \right)  = \sum_i\sum_j i j P\left( \xi = i \right) P\left( \eta = j \right)  = E_{\xi}E_{\eta}$

    $i\cdot j = \alpha\quad i\in R_{\xi}\quad j\in R_{\eta}$
\end{proof}

\begin{example}
    $\Omega = \{0,1\}\quad p = \frac{1}{2}\quad \xi(i) = 2i\quad E_{\xi} = 1$ 

    $\Omega = S_n\quad p = \frac{1}{n!}\quad \xi = $ число неподвижных точек $\quad E_{\xi} = 1$

    Матожидание одно, но ведут себя совершенно по разному.
\end{example}

\begin{definition}
    [Дисперсия]

    $D_{\xi} = Var\left( \xi \right) $ 

    $D_{xi} = E\left( \xi-E_{\xi} \right) ^2  =E\left( \xi^2 - 2\xi E_{\xi} + \left( E_{\xi} \right) ^2 \right)  = E_{xi}^2 - 2E_{\xi}E_{\xi} + \left( E_{\xi} \right) ^2 = E_{\xi^2} - \left( E_{\xi} \right) ^2$
\end{definition}

\begin{theorem}
    $D_{c\xi} = c^2D_{\xi}$

    Если $\xi$ и  $\eta$ независимы, то $D_{\xi + \eta} = D_{\xi} + D_{\eta}$
\end{theorem}
\begin{proof}
    Упражнение
\end{proof}


\begin{proof}
    [Вспомним]

    $\xi, \eta : \Omega \to  \R$

    $F_{\xi}(a) = P\left( \xi \leqslant  a \right) $

    $f_{\xi}(a) = P\left( \xi = a \right) $

    $F_{\xi}(a) = \sum_{b\leqslant a} f_{\xi}(b)$

    $E_{\xi} = \sum_{\omega\in \Omega}p(\omega)\xi(\omega)  = \sum_a a\cdot P\left( \xi = a \right) $

    $E(\xi+\eta) = E_{\xi} + E_{\eta}$
\end{proof}

$E\left( \xi - E\xi \right)  = E_{\xi} - EE_{\xi} = 0$ матожидане отклонения от матожидания равно нулю..

Хочется смотреть насколько величина отклоняется от своего матожиданя. Для этого используется понятие дисперсии:

$D_{\xi} = E\left( \xi-E\xi \right)^2 = E\xi^2 - \left( E\xi \right) ^2 $

$D\left( \xi+\eta \right)  = E\xi^2 + E\eta^2 + 2E\xi\eta - \left( E\xi \right) ^2 - \left( E\eta \right) ^2 - 2E\xi E\eta = D_{\xi} + D_{\eta} + 2\left( E_{\xi\eta} - E_{\xi}E_{\eta} \right) $. 

В случае независимых случайных величин дисперсия линейна. Иначе она отличается на ковариацию:

\begin{definition}
    [Ковариация]

     \[
     Cov\left( \xi, \eta \right)  = E_{\xi\eta} - E_{\xi}E_{\eta}
    .\]
    
    $D_{\xi} = Cov\left( \xi, \xi \right) $ 
\end{definition}

\begin{definition}
    [Корреляция]

    \[
        Corr\left( \xi, \eta \right)  = \frac{Cov\left( \xi, \eta \right) }{\sqrt{D_{\xi}D_{\eta}} } 
    .\] 
\end{definition}

\begin{theorem}
    Корреляция двух случайных величин лежит между -1 и 1.

    \[
        -1\leqslant Corr(\xi, \eta) \leqslant 1
    .\] 
\end{theorem}
\begin{proof}
    $\alpha = \xi-\lambda \eta$

    $D_{\alpha} = E_{\xi^2} - 2\lambda E_{\xi\eta} +\lambda^2 E\left(\eta^2  \right) - (E(\xi))^2 + 2\lambda E_{\xi}E_{\eta} - \lambda^2\left( E_{\eta^2} \right)\geqslant 0  $

    $D_{\xi} + 2\lambda Cov\left( \eta, \eta \right)  + \lambda^2D_{\eta}$

    $4Cov\left( \xi, \eta \right) ^2 - 4D_{\xi}D_{\eta}\leqslant 0$
\end{proof}

\section{Хвостовые неравенства}

$\xi\quad E\xi = 10\quad \xi \geqslant  0$

$P\left( \xi\geqslant 100 \right) <\frac{1}{10}$ 

\begin{theorem}
    [Неравенство Маркова]

    $\xi \not\equiv 0\quad \xi\geqslant 0\quad P\left( \xi\geqslant a\cdot E_{\xi} \right) \leqslant \frac{1}{a}$
\end{theorem}
\begin{proof}
    $E_{\xi} = \sum_{v}v\cdot P\left( \xi = v \right) = \sum_{v< a\cdot E_{\xi}}v\cdot P\left( \xi = v \right) + \sum_{v \geqslant  a\cdot E_{\xi}}v\cdot P\left( \xi=v \right) \geqslant 0+a\cdot E_{\xi}\sum_{v\geqslant a\cdot E_{\xi}}P\left( \xi = v \right) = a\cdot E_{\xi}\cdot P\left( \xi\geqslant a\cdot E_{\xi} \right)  $
\end{proof}


\begin{example}
    $a = \frac{c}{E_{\xi}}\quad P\left( \xi\geqslant c \right) \leqslant \frac{E_{\xi}}{c}$
\end{example}



$D_{\xi} = E\left( \xi-E\xi \right) ^2$

$\eta = \left( \xi-E_{\xi} \right) ^2$

$P(\left( \xi - E_{xi} \right)^2 \geqslant a^2\cdot D_{\xi}) \leqslant \frac{1}{a^2}$

$\sigma = \sqrt{D_{\xi}} $ -- среднеквадратичное отклонение
\begin{theorem}
    [Неравенство Чебышева]
$P\left( |\xi-E_{\xi}|\geqslant a\sigma \right) \leqslant \frac{1}{a^2}$
\end{theorem}

$P\left( |\xi-E_{\xi}|\geqslant c \right) \leqslant \frac{D_{\xi}}{c^2}$

\begin{figure}[ht]
    \centering
    \incfig{drawing}
    \caption{drawing}
    \label{fig:drawing}
\end{figure}

\begin{problem}[10 монет, найти количество ``1'']
    \[E\xi = 5 \quad D\xi = 2{,}5\]
    \[P(\xi \leqslant 0) \leqslant P(|\xi - E\xi| \geqslant 5) \leqslant \cfrac{2{,}5}{25} = \cfrac{1}{10}\]
\end{problem}

\begin{note}
    С одной стороны, у неравенства есть плюс: \quad оно \textbf{всегда} работает; всегда (!)
    
    С другой --- иногда оценки получаются, очень грубыми. В нашем примере ответ $\leqslant \dfrac{1}{10}$, а в жизни --- $\dfrac{1}{1024}$.
\end{note}


\begin{example}
    Нечестная монета $p\neq \frac{1}{2}$. Хотим выяснить чем чаще выпадает.

    Бросили: $c$ единиц,  $n-c$ нулей. Предположим, что  $c<\frac{n}{2}\quad p>\frac{1}{2}\quad pn>\frac{n}{2}$ 

    $P\left( \xi = c \right) \leqslant P\left( \xi\leqslant c \right) \leqslant P\left( |\xi-pn|\leqslant pn-c \right) \leqslant P\left(| \xi-pn|\leqslant \frac{n}{2}-c \right) \leqslant \frac{n}{4\cdot \left( \frac{n}{2}-c \right) ^2}$
\end{example}

\begin{theorem}
    [Граница Чернова (без доказательства)]

    $\xi_i\quad P\left( \xi_i = 1 \right) \quad P\left( \xi_i = 0 \right) =1$

    $\xi = \sum_{i=1}^{n} \xi_i\qquad E_{\xi} = np = \mu$

    $P\left( |\xi - \mu| \geqslant \delta \mu \right) < e^{-\mu \frac{\delta^2}{3}} $
\end{theorem}

\begin{example}
    Случайная величина  $\xi$. Хотим узнать матожидание. Проведём эксперимент  $n$ раз:  $\xi_1, \xi_2, \ldots, \xi_n$

   \[
       P\left( \left| \frac{\sum \xi_i}{n} - E_{\xi} \right| > c  \right) \leqslant \frac{D_{\xi}}{n\varepsilon^2} 
   .\] 

\end{example}

$\xi:\Omega \to \Z^+$

$E_{\xi} =\sum_{i=0}^{n} i\cdot P\left( \xi=i \right)  = \sum_{i=0}^{n} \left( P\left( \xi\geqslant i \right) -P\left( \xi\geqslant i+1 \right)  \right)  = \sum_{i=1}^{n} P\left( \xi\geqslant i \right) $

\section{Теория информации}

\begin{definition}
    [Что такое информации]
$ $\\
    Информация $=-$ неопределённость
\end{definition}

непределённость Н1. Что-то узнали, стала неопределённость Н2. полученная информация  $I = $Н1-Н2  $=-\Delta$ Н

Хочется убрать наблюдателя, нас, из определения, чтобы не было кого-то, кто узнаёт и меняет неопределённость. Надо ввести объективную модель:

    \begin{definition}
        [Случайный источник]
        $\Omega$ -- вероятностное простраство. Есть исходы  $p_1, p_2, \ldots, p_n$

        Чёрный ящик с красной кнопкой и дисплеем. Основан на вероятностном пространстве

        $\xi_1, \xi_2, \ldots, \xi_m \ldots$

        $P(\xi_i = a) = p_a\quad a = 1 \ldots n$
    \end{definition}

     Случайный источник $p_1, p_1, \ldots, p_n$. Хотим померять сколько информации содержится в одном результате эксперимента.

     $H\left( p_1, p_2, \ldots, p_n \right) : RS(random sources) \to  \R^+$

     Частный случай $p_i = \frac{1}{n}$ 

     $h(n) = H\left( \frac{1}{n}, \frac{1}{n}, \ldots, \frac{1}{n} \right) $ 

     \begin{enumerate}
         \item $h(n+1)>h(n)$
         \item 

             \begin{example}
                 $\Omega = \left\{ (1,1), (1, 2), \ldots, (1, m_1) (2, 1), \ldots, (2, m_2), \ldots, (k, 1), \ldots, (k, m_k) \right\} $

                 $n = m_1+m_2+\ldots m_n$

                 $p(i,j) = 1_{ij}\quad p_i = \sum_{j=1}^{m_k} q_{ij}$ 

                 Первый ряд $(1,*)$ --  $p_1$. Второй $p_2$ ... Последний $p_k$

                 Если случайный источник показывает только первое число это эквивалентно  $H(p_1, p_2, \ldots, p_n)$


                 Теперь представим, что мы сначала узнаём первую компоненту, а потом открываем вторую

                 $\sum_{i=1}^k p_i  H\left( \frac{q_{i 1}}{p_i}, \ldots, \frac{q_{i m_i}}{p_i}  \right) $ 

                 Если провести эксперимент сразу, получим $q_{11}, \ldots, q_{i m_i}$ 

                 $q_{ij} = p_i q_{ij}$
                    
             \end{example}

             \[
                 H\left( p_1r_{11}, p_{1}r_{12}, \ldots, p_1r_{1k_1}, p_2r_{21}, \ldots, p_kr_{km_k} \right) = H\left( p_1, p_2, \ldots, p_k \right)   + \sum_{i=1}^{k} p_iH\left( r_{i1}, \ldots, r_{i m_i}  \right) 
             .\] 
             
             \item Для фиксированного $n$ Н непрерывная как функция $\R^n \to \R$
     \end{enumerate}

     \begin{theorem}
         $H\left( p_1, p_2, .., p_n \right) = -\sum_{i=1}^{n} p_i \log p_i$
     \end{theorem}
     \begin{lemma}
         $h\left( nm \right)  = H_(n) + h(m)$ Следует из второго свойства
     \end{lemma}
     \begin{proof}
         $k=n\quad m_i = m\quad p_i = \frac{1}{n}\quad q_{ij} = \frac{1}{nm}\quad r_{ij} = \frac{1}{m}$ 

         $h(nm) = H\left( q_{11}, q_{12}, \ldots, q_{nm} \right)  =  H\left( \frac{1}{n}, \ldots, \frac{1}{n} \right) + \sum_{i=1}^{n} \frac{1}{n} H\left( \frac{1}{m}, \ldots, \frac{1}{m} \right) =h(n) + h(m)$
     \end{proof}

     \begin{definition}
         $h(2) = \alpha$ (может с точностью до мультипликативной константы задать)
     \end{definition}

     \begin{lemma}
         $h(2^k) = k\alpha$
     \end{lemma}

     \begin{lemma}
         $h(n) = \alpha \log_2 n$
     \end{lemma}
     \begin{proof}
         $2^{i}\leqslant n^ r < 2^{i+1}\quad r\in \N $

         $h(i)\leqslant h\left( n^r \right)  < \alpha(i+1)$

         $\alpha\cdot i \leqslant  r\cdot h(n) < \alpha(i+1)$

         $\alpha\cdot \frac{i}{r}\leqslant h(n) , \alpha \frac{i+1}{r}$ 

         $i\leqslant r\log_2 n < i+1$

         $\alpha \frac{i}{r} \leqslant \alpha \log_2 n < \alpha \frac{i+1}{r}$ 

         $\forall r\quad \left| h(n) - \alpha \log_2 n \right| \leqslant  \frac{\alpha}{r}$ 

         $\implies  h(n) = \alpha \log_2 n$
     \end{proof}

     \begin{proof}
         [Доказательство теоремы]
         Рациональный $p_i = \frac{a_i}{b}\quad m_i = a_i\quad r_{ij} = \frac{1}{a_i}\quad q_{ij} = \frac{1}{b}\quad q_{ij} = p_ir_{ij}$ 


 $H\left( \underbrace{\frac{1}{b}, \ldots, \frac{1}{b}}\limits_{b} \right) = H\left( p_1, \ldots, p_k \right) +\sum_{i=1}^{k} p_iH\left( \underbrace{ \frac{1}{a_i}, \ldots, \frac{1}{a_i}}\limits_{a_i} \right)  $ 

 $\left(\sum_{i=1}^{k}  p_i \right) h(b) = H\left( p_1, \ldots, p_k \right)  + \sum_{i=1}^{k} p_ih(a_i)$    

 $H\left( p_1, \ldots, p_k \right)  = \sum_{i=1}^{k} p_i\left( \alpha \log_2 b - \alpha \log_2 a_i \right) $

 Функция непрерывна и она верна для рациональных, следовательно она верна для всех
     \end{proof}
     \begin{note}
         $h(2) = \alpha$ -- бит
     \end{note}

     А теперь мы хотим перевести определение информации на неслучайный источник
     \begin{proof}
         [Ответ 1]
         Это тогда будет не совсем корректно с математической точки зрения. Когда смотришь на конкретные детерминированные данные.
     \end{proof}
     \begin{proof}
         [Ответ 2]
         Изучение среднего не совсем антинаучное занятие. Внешне оно ведёт себя как случайные величины.
     \end{proof}


\begin{example}
    Есть строка $s$, в которой мы хотим померить информацию.

     $s\in \Sigma^*\quad n = \left| \Sigma \right| $

     $|s| = l\quad f_i$ -- количество символов  $c_i$ в строке  $s$

      $p_i = \frac{f_i}{L}$ 

      Допустим, что символы выдаёт случайный источник, который выдал символы $s_1, s_2, \ldots , s_L$. Статистически эта строка похожа на $s$.  *натягивание на глобус* Допустим, что количество информации в строке $s$ равно количеству в строке  $\tl s$

      $I(\tl s) = J\cdot H\left( p_1, p_2, \ldots, p_n \right)  = -L \sum_{i=1}^{n} p_i \log_2p_i$
\end{example}

Вспомним арифметическое кодирование

$q=A(s)$ -- длина арифместического кодирования

$A(s)\leqslant -\log_2 (b_L-a_L) = -\log_2 \left( p_{s_1}\cdot \ldots\cdot p_{s_L} \right)  = -\log_2 \left( \prod_{i=1}^np_i^{f_i} \right) =-\sum_{i=1}^{n} \underbrace{\frac{L}{p_i}}\limits_{p_i}\log_2 p_i =\\= I\left( \tl s \right)  = L\cdot H\left( p_1, p_2, \ldots, p_n \right)   $

\begin{theorem}
    Длина кода, после арифметического кодирования не превышает энтропию Шеннона
\end{theorem}
\begin{note}
    Арифметическое кодирование асимптотически оптимально среди тех, которые не учитывают взаимное расположение символов.
\end{note}

\begin{example}
    [Нижняя оценка для сортировки]

    Пусть $a_1, \ldots, a_n$ -- перестановка и мы хотим её отсортировать

    \begin{statement}
        От одного сравнения мы получаем не больше 1 бита информации
    \end{statement}

    Рассмотрим все перествки. В каждой содержится 

    $\log_2 n! = \sum_{i=1}^{n} \log_2 i\geqslant \sum_{i=\frac{n}{2}}^{n} \log_2 \frac{n}{2} = \Omega\left( n\log n \right)  $
\end{example}

\section{Цепи Маркова}

$b = \left( b_1, b_2, \ldots, b_n \right) $ -- $b_i$ вероятность находиться в состоянии  $i$

$C\quad c = \left( c_1, c_2, \ldots, c_n \right) $ -- случайная величина после одного перехода

Матрица перехода  $p_{ij}$ -- вероятность перейти из  $i$ в  $j$

$P = \begin{bmatrix} 0&0&\frac{1}{2} &0\\0&1&0&0\\\frac{1}{2} & 0 & 0 &\frac{1}{2}\\0&0&0&1 \end{bmatrix} $

$c_i = P(C=i) = \sum_{j=1}^{n} P\left( x=i | B = j \right) P(B=j) = \sum_{j=1}^{n} p_{ji}\cdot b_j$

$b^0 = \left( 1,0,0,0 \right) $ -- нулевой шаг

$b^1 = (0,\frac{1}{2}, \frac{1}{2}, 0)$

Рассмотрим судьбу м.ц. после поглощения. Жизнь происходит внутр одной сильно связанной компоненты -- эргадического класса. 

\begin{enumerate}
    \item $d > 1$ длинна любого цикла кратна $d$. Циклический класс
    \item НОД(длин всех циклов) $= 1$. 
\end{enumerate}

\begin{theorem}
    [Эргадическая для регулярных цепей]
    М.ц. такова, что $p_{ij} >0 \forall i, j$

    Тогда $\exists\ b\quad \forall\ b^0\quad b^0P^n \to b$

    ($b$ удолветворяет равенству  $b = bP$
\end{theorem}
\begin{proof}
    $(b^0A)_i = \sum_{j=1}^{n} b^0_j\cdot A_{ji} = \left( \sum_{j=1}^{n} b_j^0 \right) \tl a_i = \tl a_i$ 

    $\sqsupset \forall j\quad a_{ji} = \tl a_i$

    $P^n \to A$, которая  удовлетворяет условию выше.

    $m_i^t = \min_j (P^t)_{ji}\quad M_i^t = \min_j (P^t)_{ji}$

    $M^t_i - m_i^t \to  0$

    $\delta = \min_{i,j}\quad \delta >0$
    
\begin{align*}
    P_{ji}^{t+1} &= \sum_{k=1}^{n} P^t_{jk}P_{ki}\\
                 &\leqslant \overbrace{\sum_{\substack{k = 1\\k\neq posMin}}^{n} P_{jk}}^{1}M_i^t + P_{j\ posMin}(m_i^t - M_i^t)\\
                 &\leqslant  M_i^t + \delta\left( m_i^t - M_i^t \right)  
.\end{align*}

Аналогично с максимумом, оцениваем всё снизу минимумов, кроме максимума 



\[
    M_i^{t+1} \leqslant  M_i^t + \delta\left( m_i^t - M_i^t \right) 
.\] 

\[
    -m_i^{t+1} \leqslant  -m_i^t + \delta\left( m_i^t - M_i^t \right) 
.\] 

\[
    M_i^{t+1} - m_i^{t+1} \leqslant  \left( M_i^t - m_i^t \right) \left( 1-2\delta \right) \leqslant \left( 1-2\delta \right) ^{t+1} \to 0
.\] 

Теперь у $b = bP$

$(I-P)b = 0$

$Rg\left( I - P \right) = n-1$

$\sum b_i = 1$ 

$P^{2^c}$

$bP^n 0> b\quad bP^{n+1} \to bP$

$b = bP$
\end{proof}

Вернёмся к вопросу что происходит после поглощения.

$\sphericalangle $ эргадический класс $A\quad \tl p = \sum_{a\in A}\left( b^oNR \right)_a $

$\tl b^0 = \left( b^0NR \right) _{A - \frac{1}{\tl p}}$ 

$\exists $ предельное $b:\quad \tl b^0 A^n \to b$

Конечное распределение $b\tl p$


Скрытые Марковские модели. Мы решали до этого прямую задачу -- брали м.ц. с извстными матрицами перехода и смотрели на их характеристики.

Есть обратная: Есть состояние и мы хотим узнать матрицу перехода.

Ещё задача: Есть немарковский процесс и мы хотим аппроксимировать его марковским.


\section{Формальные языки}


Алфавит -- $\Sigma$, конечное непустое множество

слово, цепочка, строка  $\Sigma^* = \bigcup\limits_{k=0}^{\infty }\Sigma^k$

Формальный язык $L\subseteq \Sigma^*$

<TODO>

\begin{definition}
    Описание языка -- слово конечной длины

    Всего ``описаний'' счётное множество 
\end{definition}

\begin{itemize}
    \item Распознавание -- по слову возвращаем булевский флаг -- есть слово в нашем языке или нет
    \item Порождение -- описывает как породить возможно бесконечное количество слов
\end{itemize}

\begin{definition}
    Перечиление слов.  $\left\{ 01,011,10,1010 \right\} $. Но так можно описать только конечные языки (содержащие конечное количество слов
\end{definition}

\begin{example}
    Правильные скобочные последовательности
    $\varepsilon$ -- псп

    $A,B$ -- псп  $\implies AB$ -- псп

    $A$ -- псп  $\implies (A)$ -- псп

    Это порождение. Можно описать распознованием: баланс в любой момент неотрицательный, баланс в конце $=0$
\end{example}

Между этими способми есть логический переход:
\begin{itemize}
    \item Распозновать $\to $ Порождать. порождаем всё, что можем распознать
    \item Обратно: распознаём всё, что в какой-то момент порождаем
\end{itemize}

\begin{example}
    С++ без ограничений по памяти. Пограмма $P$

    $L = \left\{\omega \mid p(\omega)=1  \right\} $

\end{example}

\subsection{Регулярные = Автоматные языки}

\begin{definition}
    Конкатенация: $\alpha\in\Sigma^k, \beta\in \Sigma^l\quad \alpha\beta\in \Sigma^{k+l}$

     $\gamma = \alpha\beta\quad \gamma_i =\begin{cases}
         i\leqslant k&\implies \alpha_i\\
         i>k&\implies \beta_{i-k}
     \end{cases}$ 

     $\alpha(\beta\gamma) = \left( \alpha\beta \right) \gamma\qquad \alpha\varepsilon = \varepsilon\alpha = \alpha$
\end{definition}

\begin{example}
    $AB = \left\{ x \mid x = yz, y\in A, z\in B \right\} $ 

    $A = \left\{ 0,01 \right\} \quad B = \left\{ 0,10 \right\} $

    $AB = \left\{ 00,010,0110 \right\} $
\end{example}

Базовые операции:
\begin{enumerate}
    \item Объединение $A\cup B$
    \item Конкатенация $AB$
    \item[] Возведение в степень  $A^k = \underbrace{AA\ldots A}_k\quad A^0 = \left\{ \varepsilon \right\} $ 
    \item Замыкание Клини $A^* = \bigcup\limits_{k=0}^{\infty }A^k$
\end{enumerate}

\begin{definition}
    $Reg_0 = \left\{ \O, \left\{ \varepsilon \right\} , \left\{c\right\} \forall c\ \in \ C \right\} $

    $Reg_{i+1} = Reg_i \cup \left\{ A\cup B, AB, A^* \mid A, B\in Reg_i \right\} $ 

    $Reg_1 = \left\{ \O , \varepsilon, a, b, \ldots, \left\{a,b\right\}, \left\{a,\varepsilon\right\}, \ldots., {ab}, {aa}, \ldots., \left\{\varepsilon, a, aa, aaa, \ldots \right\} , \ldots, \left\{ \varepsilon b, bb, bbb, \ldots \right\}\right\} $

    $Reg = \bigcup\limits_{k=0}^{\infty }Reg_k$
\end{definition}

\begin{lemma}
    $A, B\in Reg$:
     \begin{enumerate}
        \item $A\cup B\in Reg$
        \item $AB\in Reg$
        \item  $A^*\in Reg$
    \end{enumerate}

    $A\in Reg_i, B\in Reg_j$

    Они все принадлежат $Reg_{\max\{i,j\}+1}$
\end{lemma}

\begin{definition}
    Назовём семейство языков $X\in Good\quad X\subseteq 2^{\Sigma^*}$

    $X = set\left<lang \right>$ 

    Good: set<set<lang> >
    \begin{enumerate}
        \item $Reg_0\in X$
        \item $X$ замкнуто относительно  $A\cup B, AB, A^*$, a.и. \[
        A, B\in X\implies AB, A\cup B, A^*\in X\qquad A,B:lang
        .\] 
    \end{enumerate}
\end{definition}

\begin{theorem}
    $Reg = \bigcap\limits_{u\in Good}U $
\end{theorem}
\begin{proof}
    to be written
\end{proof}

\begin{definition}
    [Описание]
    \begin{itemize}
        \item $\O \quad \varepsilon\quad c$
        \item $A\ \alpha\quad B\  \beta$:
             \begin{itemize}
                \item $AB\quad \alpha\beta$ -- средний приоритет
                \item $A \cup B\quad \alpha |\beta$ -- минимальный приоритет
                \item $A^*\quad \alpha^*$ -- максимальный приоритет
            \end{itemize}
    \end{itemize}

    $\Sigma = \left\{ 0,1 \right\} $ 

    $\left( 0\mid 11 \right)^* $ -- язык в кортором единицы идут парами

    Такие описания называют академическими регулярными выражениями

    $\alpha^+ = \alpha\alpha^*$

    $\alpha^k = \underbrace{\alpha\alpha \ldots \alpha}_{k}$
\end{definition}

\begin{example}
    $0^*|\left( 0^*10^*10^* \right)^*$ -- язык, содержащий чётное число единиц
\end{example}

\begin{example}
    Проверка чётное ли число единиц

\begin{figure}[!ht]
    \centering
    \incfig{check-one}
    \caption{check-one}
    \label{fig:check-one}
\end{figure}

\begin{figure}[!ht]
    \centering
    \incfig{check-div3}
    \caption{check-div3}
    \label{fig:check-div3}
\end{figure}
\end{example}

\begin{definition}
    Детерминированный Конечный Автомат ДКА DFA

    \[
    A = \left<\Sigma, Q, S\in \Sigma, T\subseteq Q, \delta:Q\times \Sigma \to Q \right>
    .\] 

    \begin{itemize}
        \item $\Sigma$ -- алфавит
        \item Q -- конечное множество состояний
        \item  $S$ -- начальное состояние
        \item  $T$ -- допускающие состояния
        \item  $\delta$ -- функция переходов
    \end{itemize}

    $Snap = Q\times \Sigma^*$

    Пееход:
     \begin{tikzpicture}
        
    \end{tikzpicture}
    \begin{enumerate}
        \item $\alpha = c\beta\quad c\in \Sigma$
        \item  $r = \delta(q,c)$
    \end{enumerate}
\end{definition}

\begin{example}
    $\left<e,0101 \right> \vdash \left<e,101 \right> \vdash \left<o,01 \right> \vdash \left<o,1 \right> \vdash \left<e,\varepsilon \right>$
\end{example}

$\mathscr{L}(A) = \left\{ \omega \mid \left<s, \omega \right> \vdash^* \left<t, \varepsilon \right>, t\in T \right\} $

\begin{theorem}
    [Клини] $Reg = Aut$

    $Aut = \left\{ X\mid \exists \text{ ДКА } A:\quad X = L(A) \right\} $
\end{theorem}

\section{Недетерминированный конечный автомат}

$x$ -- допускаюется Недетерминированным Конечным Автоматом $\iff \exists $ последовательность переходов по символам $x$, заканчиващееся в допускающем состоянии

$L$ -- формальный язык.  $L\times \Sigma^*$

Артур: $x \mapsto x\in L?$

Мерлин: Убедить Арутра, что $x\in L$

\begin{note}
   Артуру в случае неопределённости выгодно слушать Мерлина.

   Если $x\not\in L$, то Мерлин не сможет испортить своими советами, потому что в автомате просто нет такой последовательности, на которую можно направить, чтобы попасть в допускающее

   Если $x\in L$, то, внезпано, интересы Артура и Мерлина совпадают
\end{note}

\begin{note}
    [интерпретация через миры]

    На каждом шаге, где недетерминирован следующий шаг, создаётся два мира, на каждый из шагов. Если хотя бы в одном дошли до допускающего, то слово принадлежит.
\end{note}

\begin{definition}
    [НКА]

    $\left( \Sigma, Q, S\subseteq Q, T\subseteq Q, \delta:Q\times E \to 2^{Q} \right) $ 

    Стартовых состояний может быть несколько, хотя почти никогда не нужно


    Состояние -- $\left<q, x \right>\quad q\in Q,\quad x\in X^*$

    $\left<q,x \right>\vdash \left<r,y \right>$

    \begin{enumerate}
        \item $x = cy,\quad c\in \Sigma$
        \item  $r\in\delta\left( q,c \right) $
    \end{enumerate}

    $x$ -- допускается  $A$, если  $\left<s,x \right>\vdash^* \left<t,\varepsilon \right>,\quad t\in T$
\end{definition}

\begin{example}
\begin{figure}[!ht]
    \centering
    \incfig{auto}
    \caption{auto}
    \label{fig:auto}
\end{figure}
\end{example}

\begin{verbatim}
    class DFA {
        // 0 .. n-1 -- Q; 0 .. c-1 -- Sigma
        s : int
        t :  vector<bool>(n)
        delta: vector<vector<int>> (n,c)

        bool accept(x) {
            cur = s
            for (i = 0 .. len(x) - 1)
                cur = delta[cur][x[i]
            return t[cur]
        }
    }   
\end{verbatim}

$O(len(x))$
\section{Динамическое программирование}

\begin{verbatim}
    class NFA {
        // 0 ... n-1 -- Q; 0 .. c-1 -- Sigma
        s : int
        t : vector<bool>(n)
        delta : vector<vector<set<int>>>(int)

        can[i][q] -- можно ли прочитав i символов x'a оказаться в состоянии q

        bool accept(x):
            can[0][s] = true
            for (i = 0 .. len(x) - 1)
                for (q = 0 .. n-1)
                    if (can[i][q])
                       for (r : delta[q][x[i])
                        can[i+1][r] = true
            for (q = 0 .. n-1)
                if (can[len(x)][q] &&  t[q])
                    return true
    }   
\end{verbatim}

$O\left( len(x) \cdot  (n^2 + m) \right) $


\begin{figure}[!ht]
    \centering
    \incfig{nfa-step}
    \caption{nfa-step}
    \label{fig:nfa-step}
\end{figure}

\begin{statement}
    Для $L\ \exists $ НКА $A_n \iff $ для $L\ \exists $ ДКА $A_D$
\end{statement}
\begin{proof}
    $\forall $ ДКА является частным случаем НКА

    \begin{itemize}
        \item [$\impliedby $] очевидно
        \item [$\implies $] Алгоритм Томпсона

\begin{figure}[!ht]
    \centering
    \incfig{tomps}
    \caption{tomps}
    \label{fig:tomps}
\end{figure}

\begin{verbatim}
    next(a : vector<bool>, c) vector<bool> 
        res = vector<bool>(n)
        for (q = 0 .. n-1)
            if a[q]
                for r : delta[q][c]
                    res[r] = true
        return res

    bool accept(x)
        can[0][s] = true
        for (i = 0 .. len(x) - 1)
            can[i+1] = next(can[i], x[i])
\end{verbatim}

$(\Sigma, Q_D = 2^{Q_N}, \{s\}, T_D = \{A\mid A\cap T_n \neq \O \} , \delta_D(A,c) = \{r \mid \exists q\in A,\ r\in \delta_N(q,c)\}$
    \end{itemize}
\end{proof}

\begin{note}
    Казалось бы, вот Детерминированный автоматы такие хорошие, зачем нужны другие? Но он имеет экспоненциальное количество состояний (верхняя оценка) по сравнению с Недерминированным

    Но в реальности можно улучшать, убирая состояния, которые недостижимы (например $\{B,C\}$ может не встречаться одновременно никогда) (в недерминированном)

    Иначе можно начать со стартовых состояний и делать очередь всех состояний, в которых мы можем быть. Именно такую конструкцию обычно и называют \underline{Алгоритмом Томпсона}.

    Конструкция описанная выше называется \underline{Конструкцией подмонжеств}.
\end{note}

\section{$\varepsilon$-НКА}

Разрешим на переходе писать не символ, а $\varepsilon$. Переходя по нему строка на входе не меняется

\begin{example}
\begin{figure}[!ht]
    \centering
    \incfig{epsauto}
    \caption{epsauto}
    \label{fig:epsauto}
\end{figure}

$\left( \left( 0|1 \right) ^*00|\left( 0|1 \right) ^*11 \right) \left( 0|1 \right) ^*$

 $0^*1^*2^*$
\end{example}

\begin{statement}
    $\forall \varepsilon$-НКА $\exists $ эквивалентный НКА без $\varepsilon$ переходов
\end{statement}
\begin{proof}
    \begin{enumerate}
        \item Рассмотрим граф $\varepsilon$-переходов. Этому графу мы сделаем транзитивное замыкание. И добавим новые рёбра как $\varepsilon$-переходы в наш граф

\begin{figure}[!ht]
    \centering
    \incfig{epsgraph}
    \caption{epsgraph}
    \label{fig:epsgraph}
\end{figure}

Язык не поменялся. Вместо прохождения по новому переходу, можно делать $n$ эпсилон-переходов в старом графе
        \item Для каждой конструкции: Из $p$ есть  $\varepsilon$-переход в $q$ терминальный, сделаем  $p$ тоже терминальным

             \begin{statement}
                Если $x$ Допускалось раньше, $\iff $ допускается и сейчас. Последний переход не $\varepsilon$
            \end{statement}
        \item Рассмотрим тройки вершин, что Из  $p$ есть  $\varepsilon$ переход в $q$ откуда переход по  $c$ в  $r$. Тогда добавим ребро из  $p$ в  $r$ по  $c$

            \begin{statement}
                Если слово можно допустить, то его можно допустить вообще не делая $\varepsilon$-переход
            \end{statement}
        \item удалим все $\varepsilon$-переходы 
    \end{enumerate}
\end{proof}
\begin{figure}[!ht]
    \centering
    \incfig{proof-pic}
    \caption{proof-pic}
    \label{fig:proof-pic}
\end{figure}

\begin{statement}
    Для любого Языка следующие три утверждения эквивалентны:
    \begin{enumerate}
        \item Можно построить ДКА
        \item Можно построить НКА 
        \item Можно построить $\varepsilon$-НКА
    \end{enumerate}

    $2 \implies 1$ -- Томпсон

    $3 \implies 1$ -- $\varepsilon$-замыкание
\end{statement}



\begin{theorem}
    [Клини]
    $\Reg = \Aut$
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item []
        \item [$\Reg \subseteq  \Aut$] Докажем по индекции, что $\forall i\quad \Reg_i\subseteq Aut$

            Будем строить $\varepsilon$-НКА с одним терминальным состоянием

            База: $\O $. Стратовое и терминальное состояние и никаких переходов

            $\varepsilon$ -- одна $\varepsilon$-стрелка

            $c$ -- одна  $c$-стрелка

            Переход:  $\Reg_i \subseteq Aut \implies \Reg_{i+1}\subseteq Aut$

            \begin{itemize}
                \item $L = A\cup B$ 
                \item $L = AB$
                \item  $L = A^*$
            \end{itemize}

\begin{figure}[!ht]
    \centering
    \incfig{move-klini}
    \caption{move-klini}
    \label{fig:move-klini}
\end{figure}
    \end{enumerate}
\end{proof}













\end{document}
